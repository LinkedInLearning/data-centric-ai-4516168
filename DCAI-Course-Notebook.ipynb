{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install tensorflow-data-validation\n",
    "!pip install eli5\n",
    "!pip install lime\n",
    "!pip install tf-explain\n",
    "!pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import eli5\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from fairlearn.metrics import selection_rate\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample maternal health risk dataset\n",
    "health_risk_data = pd.read_csv(\"./Maternal Health Risk Data Set.csv\")\n",
    "print(health_risk_data.shape)\n",
    "health_risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X = health_risk_data.drop('RiskLevel', axis=1)  # Features\n",
    "y = health_risk_data['RiskLevel']  # Labels\n",
    "\n",
    "\n",
    "# Split with a 70-30 train-eval ratio (you can adjust this ratio)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data originally doesn't contain any missing values or anomalies, we are introducing those data issues to show you how to treat them\n",
    "for col in X_train.columns:\n",
    "  X_train.loc[X_train.sample(frac=0.05).index, col] = np.nan\n",
    "\n",
    "for col in X_val.columns:\n",
    "  X_val.loc[X_val.sample(frac=0.05).index, col] = np.nan\n",
    "\n",
    "# Introduce anomaly values in Age feature\n",
    "X_val.loc[X_val.sample(frac=0.01).index, \"Age\"] *= 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Descriptive Statistics for Train and Val dataset\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(X_train)\n",
    "val_stats = tfdv.generate_statistics_from_dataframe(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric columns. We are not including the Target variable here. This will help us understand if there is any multicollinearity\n",
    "\n",
    "correlation_matrix = X_train.corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above correlation map you can see that SystolicBP is highly correlated with DiastolicBP. Looking at this from a clinical lens, we understand that blood pressure is measured using two valuesâ€”systolic and diastolic. SystolicBP represents the maximum arterial pressure during a cardiac cycle when the heart is contracting (systole), while DiastolicBP represents the minimum arterial pressure when the heart is at rest (diastole). The two values are obtained during the same blood pressure measurement, hence the correlation.\n",
    "\n",
    "As you can see in the above two desctiptive stats from the train and validation dataset, that there are missing values in each of the columns. In the next section, we will learn to impute data in these missing positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our dataset, all the feature values are numeric, hence we will use the same imputation method across all feature.\n",
    "# If your dataset has a mix of categorical, text, and numerical values, then you will need to use different methods for data imputations.\n",
    "# In our dataset as we used a random method to introduce missing values, there is no pattern behind missing values.\n",
    "# But in other cases, there might be a reason why certain values might be missing, and in those cases you will need to treat them differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in numeric columns of training data with mean\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "X_train[X_train.columns] = numeric_imputer.fit_transform(X_train)\n",
    "\n",
    "# Imputing values in validation dataset\n",
    "# Imputing data in validation is different than train data. \n",
    "# In train you do fit_transform, which means you are deriving the value of the mean from that data and imputing in the same data.\n",
    "# Whereas, you shouldn't be deriving the imputation metric from the validation or test data, hence we use .transform for val and test dataset.\n",
    "\n",
    "X_val[X_val.columns] = numeric_imputer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting outliers or anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Outlier detection using Z-Score\n",
    "outliers_zscore = {}\n",
    "threshold = 3  # Adjust the threshold as needed\n",
    "for column in X_train.columns:\n",
    "    z_scores = np.abs(stats.zscore(X_train[column]))\n",
    "    outliers_zscore[column] = X_train[column][z_scores > threshold]\n",
    "\n",
    "print(\"Outliers detected using Z-Score for X_train:\")\n",
    "for column, outliers in outliers_zscore.items():\n",
    "    print(f\"{column}: {list(outliers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Outlier detection using IQR (Interquartile Range)\n",
    "outliers_iqr = {}\n",
    "for column in X_train.columns:\n",
    "    Q1 = X_train[column].quantile(0.25)\n",
    "    Q3 = X_train[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers_iqr[column] = X_train[column][(X_train[column] < lower_bound) | (X_train[column] > upper_bound)]\n",
    "\n",
    "print(\"Outliers detected using IQR:\")\n",
    "for column, outliers in outliers_iqr.items():\n",
    "    print(f\"{column}: {list(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Infer Schema for Train and Eval dataset\n",
    "train_schema = tfdv.infer_schema(train_stats)\n",
    "eval_schema = tfdv.infer_schema(val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = schema_pb2.FloatDomain(name=\"Age\", min=1, max=90) # creating domain buffer\n",
    "tfdv.set_domain(schema=train_schema,feature_path=\"Age\",domain=dom) # setting domain\n",
    "tfdv.display_schema(train_schema)\n",
    "\n",
    "# Similarly you can edit the schema and specify details about how the ideal data would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate Anomalies in Eval data compared to the Train data\n",
    "# In this cell you will see that the anomalies we introduced with the Age Feature is detected.\n",
    "\n",
    "anomalies = tfdv.validate_statistics(val_stats, train_schema)\n",
    "tfdv.display_anomalies(anomalies)\n",
    "\n",
    "# In the cell below you can see that the values in the Age field is not the same as we found in the train schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare evaluation data with training data\n",
    "tfdv.visualize_statistics(lhs_statistics=val_stats, rhs_statistics=train_stats,\n",
    "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our use-case we will be using skew_comparator and it is appropriate for our use-case of comparing the data distribution of train and eval dataset.\n",
    "# In this cell we are using Jensen Shannon Divergence Metric to evaluate the skew for \"Age\" feature in the dataset. \n",
    "# The threshhold is determined using statistical method that is not covered in this course.\n",
    "# The recommendation is for the learners to replicate similar tests for other features, and document their observations about the data they are working with.\n",
    "\n",
    "tfdv.get_feature(train_schema, 'Age').skew_comparator.jensen_shannon_divergence.threshold = 0.01\n",
    "skew_anomalies = tfdv.validate_statistics(\n",
    "  statistics=train_stats,\n",
    "  schema=train_schema,\n",
    "  serving_statistics=val_stats)\n",
    "tfdv.display_anomalies(skew_anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "Now lets build a simple classifier model for the above dataset, and then we will see a few methods for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a classification algorithm (Random Forest in this example)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy for Holdout Validation\n",
    "accuracy_holdout = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Calculate other evaluation metrics\n",
    "confusion_matrix_holdout = confusion_matrix(y_val, y_pred)\n",
    "classification_report_holdout = classification_report(y_val, y_pred)\n",
    "\n",
    "# Print the results for Holdout Validation\n",
    "print(\"Holdout Validation Results:\")\n",
    "print(\"Accuracy:\", accuracy_holdout)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_holdout)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_holdout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf = model.predict(X_val_kf)\n",
    "    accuracy_kf = accuracy_score(y_val_kf, y_pred_kf)\n",
    "    accuracies.append(accuracy_kf)\n",
    "\n",
    "# Print K-Fold Cross Validation Results\n",
    "print(\"K-Fold Cross Validation Results (Accuracies):\")\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    print(f\"Fold {i + 1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Validation (using Random Forest)\n",
    "n_bootstrap_samples = 100\n",
    "bootstrap_accuracies = []\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    indices = np.random.choice(X_train.index, size=len(X_train), replace=True)\n",
    "    X_bootstrap = X_train.loc[indices]\n",
    "    y_bootstrap = y_train.loc[indices]\n",
    "    model.fit(X_bootstrap, y_bootstrap)\n",
    "    y_pred_bootstrap = model.predict(X_val)\n",
    "    accuracy_bootstrap = accuracy_score(y_val, y_pred_bootstrap)\n",
    "    bootstrap_accuracies.append(accuracy_bootstrap)\n",
    "\n",
    "# Print Bootstrap Validation Results\n",
    "print(\"Bootstrap Validation Results (Accuracies):\")\n",
    "for i, accuracy in enumerate(bootstrap_accuracies):\n",
    "    print(f\"Sample {i + 1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your target variable has three classes, the ROC curve is not directly applicable because ROC curves and AUC (Area Under the Curve) are typically used for binary classification problems. However, you can calculate and visualize the ROC and AUC for each class separately in a one-vs-all fashion (also known as one-vs-rest) or use another suitable metric like a multiclass version of AUC, which is often referred to as the \"micro-average\" AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each class on the testing data\n",
    "y_probs = model.predict_proba(X_val)\n",
    "\n",
    "# Define a mapping from text class labels to integer labels\n",
    "class_mapping = {'high risk': 0, 'low risk': 1, 'mid risk': 2}\n",
    "\n",
    "# Convert the text class labels in y_test to integer labels\n",
    "y_train_int = y_train.map(class_mapping)\n",
    "y_val_int = y_val.map(class_mapping)\n",
    "y_pred_int = np.array([class_mapping[p] for p in y_pred])\n",
    "\n",
    "\n",
    "# Calculate and plot the ROC curve and AUC for each class\n",
    "n_classes = len(class_mapping)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_int == i, y_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot the ROC curves for each class\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Multiclass Classification')\n",
    "plt.legend(loc='lower right')\n",
    "print(class_mapping)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above range of metrics, you need to understand if the model performance satisfies the business need.\n",
    "\n",
    "# Model Explainability and Interpretability\n",
    "\n",
    "In this next section, we will be covering Explainability and Interpretability of the model we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(X_train.to_numpy(), mode=\"classification\")\n",
    "\n",
    "# Define a sample prediction function\n",
    "def predict_function(x):\n",
    "    return model.predict_proba(x)\n",
    "\n",
    "# Select a sample instance for explanation\n",
    "sample_instance = X_val.iloc[1]\n",
    "\n",
    "# Explain the prediction for the sample instance\n",
    "explanation = explainer.explain_instance(sample_instance, predict_function, num_features=5)\n",
    "# Print the explanation\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of multiclass classification (more than two classes), \"NOT 1\" indicates that the explanation is for a class other than Class 1. It helps in understanding why a particular instance is classified as a specific class when it's not the first class in the class labels.\n",
    "\n",
    "* LIME fits simple linear models locally around a prediction to approximate the behavior of complex models like Random Forests. This provides local explainability.\n",
    "\n",
    "* The LIME output shows the top features that influenced a particular prediction, along with the weight of their contribution.\n",
    "\n",
    "* Positive weights increased the prediction probability, negative weights decreased it. Higher magnitude means more important.\n",
    "\n",
    "For this sample, we see maternal age and previous C-sections were the main drivers of the risk prediction. LIME provides instance-level explanations for model predictions. It helps explain why the model made a specific decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(model, X_val.iloc[1],\n",
    "                    feature_names=list(X_train.columns),\n",
    "                    show_feature_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(model, X_val.iloc[2],\n",
    "                    feature_names=list(X_train.columns),\n",
    "                    show_feature_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Permutation Importance explainer\n",
    "perm = PermutationImportance(model, random_state=42)\n",
    "\n",
    "# Fit the explainer to your test data (e.g., X_test, y_test)\n",
    "perm.fit(X_val, y_val)\n",
    "\n",
    "# Display feature importances\n",
    "permutation_importance = eli5.show_weights(perm, feature_names=X_val.columns.tolist())\n",
    "permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Detection and Mitigation\n",
    "\n",
    "In this section we will be seeing if the model we have built has any bias, and then we will try to mitigate it\n",
    "As FairLearn Package doesn't support multi-class target variable evaluation, we will do a slight modification in our data. We will make our target variable binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to map low and medium risk in one bucket and keep high risk as another bucket\n",
    "binary_class_mapping = {'high risk': 1, 'low risk': 0, 'mid risk': 0}\n",
    "y_binary = y.map(binary_class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a 70-30 train-eval ratio (you can adjust this ratio)\n",
    "X_train_bin, X_val_bin, y_train_bin, y_val_bin = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
    "\n",
    "bin_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "bin_model.fit(X_train_bin, y_train_bin)\n",
    "y_pred_bin = bin_model.predict(X_val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age groups\n",
    "age_groups = {\n",
    "    \"young\": (0, 18),\n",
    "    \"adult\": (19, 59),\n",
    "    \"senior\": (60, np.inf)\n",
    "}\n",
    "\n",
    "# Calculate selection rates by age group\n",
    "selection_rates = {}\n",
    "for age_group, age_range in age_groups.items():\n",
    "    age_mask = X_val_bin['Age'].between(age_range[0], age_range[1])\n",
    "    selection_rates[age_group] = selection_rate(y_val_bin[age_mask], y_pred_bin[age_mask])\n",
    "\n",
    "# Check for disparate impact in selection rates\n",
    "disparate_impact = max(selection_rates.values()) - min(selection_rates.values())\n",
    "print(\"Disparate Impact by Age Group:\")\n",
    "print(disparate_impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a multiclass target column, mitigation techniques need to be adapted accordingly. One approach to mitigate bias in multiclass classification is to address individual class disparities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Disparate Impact value in the fairness_metrics , which is calculated using the selection_rate fairness metric, means that, on average, there is a x% difference in the selection rates between the subgroups defined by the 'Age' sensitive feature.\n",
    "\n",
    "In the context of selection_rate, a value of x represents a disparity in the proportion of positive predictions (selected instances) between different age groups. This means that, on average, one age group is being selected at a rate x% higher (or lower) than another age group.\n",
    "\n",
    "The specific interpretation of this disparity depends on the context of your application and fairness requirements. A value of x suggests that there is some level of imbalance or bias in the selection rates across different age groups, but whether this is considered acceptable or not depends on the specific fairness goals and the impact of such disparities on your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current stable version of FairLearn 0.9.0 doesn't support multiclass classification. Hence I have tried to make this a binary classification problem to show how the bias mitigation would look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a disparate impact remover with Demographic Parity constraint\n",
    "di_remover = ExponentiatedGradient(\n",
    "    estimator=bin_model,  \n",
    "    constraints=DemographicParity()\n",
    ")\n",
    "\n",
    "# Train the remover on the training data\n",
    "di_remover.fit(X_train_bin, y_train_bin, sensitive_features=X_train_bin['Age'])\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred_fair = di_remover.predict(X_val_bin)\n",
    "\n",
    "# Calculate selection rates for age groups after mitigation\n",
    "selection_rates_fair = {}\n",
    "for age_group, age_range in age_groups.items():\n",
    "    age_mask = X_val_bin['Age'].between(age_range[0], age_range[1])\n",
    "    selection_rates_fair[age_group] = selection_rate(y_val_bin[age_mask], y_pred_fair[age_mask])\n",
    "\n",
    "# Check for disparate impact in selection rates after mitigation\n",
    "disparate_impact_fair = max(selection_rates_fair.values()) - min(selection_rates_fair.values())\n",
    "print(\"Disparate Impact by Age Group (After Mitigation):\")\n",
    "print(disparate_impact_fair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see in the above cell the Disparate Impact score has reduced after bias mitigation. Remember this is example is just to show you how you can model your problem. In reality, there are many qualitative assessments that needs to be done, and requires subject matter experts and social science researchers to help with understand what features in the data can cause bias, and what would be the right metric to evaluate your model, and to event decide if to what extend of reduction in the metric value is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright @aishgrt 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
